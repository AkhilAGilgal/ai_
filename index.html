<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Learning a Polynomial Function with SGD & ReLU</title>
    
    
    <link rel="stylesheet" href="style.css">
    
   
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>

    <div class="container">
        <h1>Learning a Polynomial Function with SGD & ReLU</h1>
        <p>This page demonstrates how to use key machine learning concepts—<strong>Stochastic Gradient Descent (SGD)</strong> and the <strong>ReLU</strong> activation function—to approximate a polynomial function. Instead of directly solving for <em>x</em> in a given equation, we train a simple neural network to learn the relationship between <em>x</em> and <em>y</em> from a set of data points.</p>
        <p>Our "true" function is <strong>y = 2x³ - 1x² - 5x + 3</strong>. The goal is for our model to learn this shape without ever being told the true coefficients (2, -1, -5, 3).</p>
    </div>

    <div class="container">
        <h2>The Python/PyTorch Code</h2>
        <p>Below is the Python code using the PyTorch library that performs the training. The interactive simulation on this page is based on the output of this exact code. You can run this yourself in an environment like Google Colab.</p>
        <pre><code>
# 1. Import Libraries
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# 2. Define the True Function and Generate Data
# This is the function we want our model to learn.
def true_func(x):
    # True parameters: a=2, b=-1, c=-5, d=3
    return 2 * (x**3) - 1 * (x**2) - 5 * x + 3

# Generate some (x, y) data points from the true function
N_SAMPLES = 100
X = torch.linspace(-3, 3, N_SAMPLES).unsqueeze(1)
y = true_func(X) + torch.randn(N_SAMPLES, 1) * 3 # Add some noise

# 3. Define the Neural Network Model
class PolynomialApproximator(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(1, 64)
        self.activation = nn.ReLU()
        self.layer2 = nn.Linear(64, 1)

    def forward(self, x):
        x = self.layer1(x)
        x = self.activation(x)
        x = self.layer2(x)
        return x

# 4. Set up the Training Process
model = PolynomialApproximator()
loss_function = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)

# 5. The Training Loop
epochs = 5000
for epoch in range(epochs):
    y_pred = model(X)
    loss = loss_function(y_pred, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 500 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
        </code></pre>
    </div>

    <div class="container">
        <h2>Interactive Simulation</h2>
        <p>Click the button below to simulate the training process. This will generate the initial noisy data and then plot the function our model learned after 5000 training steps (epochs).</p>
        <button id="runButton">Run Simulation</button>
        <div id="output" class="output" style="display:none;">
            <p><strong>Simulation Log:</strong></p>
            <pre id="log"></pre>
        </div>
        <canvas id="myChart"></canvas>
    </div>

    
    <script src="script.js"></script>

</body>
</html>
