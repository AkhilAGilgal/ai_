{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Polynomial Function with SGD & ReLU\n",
    "\n",
    "This notebook demonstrates how to use key machine learning concepts—**Stochastic Gradient Descent (SGD)** and the **ReLU** activation function—to approximate a polynomial function. Instead of directly solving for *x* in a given equation, we train a simple neural network to learn the relationship between *x* and *y* from a set of data points.\n",
    "\n",
    "Our \"true\" function is $y = 2x^3 - x^2 - 5x + 3$. The goal is for our model to learn this shape without ever being told the true coefficients (2, -1, -5, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Python/PyTorch Code\n",
    "\n",
    "Below is the Python code using the PyTorch library that performs the training. We will define the true function, generate noisy data, build a simple neural network, and then train it to learn the mapping from X to Y. Finally, we'll visualize the results to see how well our model learned the underlying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Define the True Function and Generate Data\n",
    "# This is the function we want our model to learn.\n",
    "def true_func(x):\n",
    "    # True parameters: a=2, b=-1, c=-5, d=3\n",
    "    return 2 * (x**3) - 1 * (x**2) - 5 * x + 3\n",
    "\n",
    "# Generate some (x, y) data points from the true function\n",
    "N_SAMPLES = 100\n",
    "X = torch.linspace(-3, 3, N_SAMPLES).unsqueeze(1)\n",
    "y = true_func(X) + torch.randn(N_SAMPLES, 1) * 3 # Add some noise\n",
    "\n",
    "# 3. Define the Neural Network Model\n",
    "class PolynomialApproximator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # A single input feature (x), and a hidden layer with 64 neurons\n",
    "        self.layer1 = nn.Linear(1, 64)\n",
    "        # ReLU activation function introduces non-linearity\n",
    "        self.activation = nn.ReLU()\n",
    "        # A single output feature (y)\n",
    "        self.layer2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# 4. Set up the Training Process\n",
    "model = PolynomialApproximator()\n",
    "loss_function = nn.MSELoss() # Mean Squared Error is a good choice for regression\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001) # Stochastic Gradient Descent\n",
    "\n",
    "# 5. The Training Loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: compute predicted y by passing x to the model\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss periodically to track progress\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# 6. Visualize the Results\n",
    "print(\"Plotting results...\")\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get the model's final predictions on the input data\n",
    "with torch.no_grad(): # We don't need to track gradients for visualization\n",
    "    learned_y = model(X)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot the original noisy data points\n",
    "plt.scatter(X.numpy(), y.numpy(), color='orange', label='True Data (with noise)', s=20, alpha=0.7)\n",
    "\n",
    "# Plot the true underlying function (without noise)\n",
    "plt.plot(X.numpy(), true_func(X).numpy(), 'g--', label='True Function', linewidth=2)\n",
    "\n",
    "# Plot the function our model learned\n",
    "plt.plot(X.numpy(), learned_y.numpy(), color='blue', label='Learned Function (Model Prediction)', linewidth=3)\n",
    "\n",
    "# Add titles and labels for clarity\n",
    "plt.title('Model Performance: Fitting a Cubic Function', fontsize=16)\n",
    "plt.xlabel('X Value', fontsize=12)\n",
    "plt.ylabel('Y Value', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
