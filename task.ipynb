import torch
import torch.nn as nn
import matplotlib.pyplot as plt

def true_func(x):
    return 2 * (x**3) - 1 * (x**2) - 5 * x + 3

N_SAMPLES = 100
X = torch.linspace(-3, 3, N_SAMPLES).unsqueeze(1)
y = true_func(X) + torch.randn(N_SAMPLES, 1) * 3

class PolynomialApproximator(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(1, 64)
        self.activation = nn.ReLU()
        self.layer2 = nn.Linear(64, 1)

    def forward(self, x):
        x = self.layer1(x)
        x = self.activation(x)
        x = self.layer2(x)
        return x

model = PolynomialApproximator()
loss_function = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)

epochs = 5000
for epoch in range(epochs):
    y_pred = model(X)
    loss = loss_function(y_pred, y)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 500 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

print("\nTraining finished.")
print("Plotting results...")

model.eval()

with torch.no_grad():
    learned_y = model(X)

plt.figure(figsize=(12, 7))
plt.scatter(X.numpy(), y.numpy(), color='orange', label='True Data (with noise)', s=20, alpha=0.7)
plt.plot(X.numpy(), true_func(X).numpy(), 'g--', label='True Function', linewidth=2)
plt.plot(X.numpy(), learned_y.numpy(), color='blue', label='Learned Function (Model Prediction)', linewidth=3)
plt.title('Model Performance: Fitting a Cubic Function', fontsize=16)
plt.xlabel('X Value', fontsize=12)
plt.ylabel('Y Value', fontsize=12)
plt.legend()
plt.grid(True)
plt.show()
